{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/alexhofman/.cache/kagglehub/datasets/fatihkgg/affectnet-yolo-format/versions/2\n",
      "Replaced with local data.yaml from the emotion_recognition folder.\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Download the dataset\n",
    "try:\n",
    "    base_path = kagglehub.dataset_download(\"fatihkgg/affectnet-yolo-format\")\n",
    "    print(\"Path to dataset files:\", base_path)\n",
    "except Exception as e:\n",
    "    print(\"Error downloading dataset:\", e)\n",
    "    exit()\n",
    "\n",
    "# Path to the Kaggle dataset's data.yaml\n",
    "kaggle_data_yaml_path = os.path.join(base_path, \"YOLO_format/data.yaml\")\n",
    "\n",
    "# If the file already exists from the downloaded dataset, remove it\n",
    "if os.path.exists(kaggle_data_yaml_path):\n",
    "    os.remove(kaggle_data_yaml_path)\n",
    "    print(\"Removed the original data.yaml from the Kaggle dataset.\")\n",
    "\n",
    "# Path to your custom data.yaml in the emotion_recognition folder\n",
    "local_data_yaml_path = \"./data.yaml\"\n",
    "\n",
    "# Copy your custom data.yaml to the dataset folder\n",
    "shutil.copy(local_data_yaml_path, kaggle_data_yaml_path)\n",
    "print(\"Replaced with local data.yaml from the emotion_recognition folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 17101/17101 [03:12<00:00, 88.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing train. Skipped 17101 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|██████████| 5406/5406 [00:20<00:00, 260.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing val. Skipped 5406 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|██████████| 2755/2755 [00:05<00:00, 532.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing test. Skipped 2755 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import csv\n",
    "import uuid  # for unique filenames\n",
    "\n",
    "def yolo_to_xyxy(label_line, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert YOLO label line (class_id, x_center, y_center, width, height)\n",
    "    from normalized coords [0-1] to absolute pixel coords [x_min, y_min, x_max, y_max].\n",
    "    \"\"\"\n",
    "    parts = label_line.strip().split()\n",
    "    class_id = int(parts[0])\n",
    "    x_center = float(parts[1]) * img_width\n",
    "    y_center = float(parts[2]) * img_height\n",
    "    w = float(parts[3]) * img_width\n",
    "    h = float(parts[4]) * img_height\n",
    "\n",
    "    x_min = int(x_center - w / 2)\n",
    "    y_min = int(y_center - h / 2)\n",
    "    x_max = int(x_center + w / 2)\n",
    "    y_max = int(y_center + h / 2)\n",
    "\n",
    "    # Clip to image boundaries\n",
    "    x_min = max(0, x_min)\n",
    "    y_min = max(0, y_min)\n",
    "    x_max = min(img_width, x_max)\n",
    "    y_max = min(img_height, y_max)\n",
    "\n",
    "    return class_id, x_min, y_min, x_max, y_max\n",
    "\n",
    "\n",
    "def process_split(yaml_dict, split_name, out_dir, csv_writer):\n",
    "    \"\"\"\n",
    "    Process one split (train, val, or test).\n",
    "    - yaml_dict: loaded from data.yaml\n",
    "    - split_name: 'train', 'val', or 'test'\n",
    "    - out_dir: where to store all cropped images for this split\n",
    "    - csv_writer: CSV writer to record (filename, class_id) rows\n",
    "    \"\"\"\n",
    "\n",
    "    images_dir = yaml_dict[split_name]  # e.g. \"/path/to/train/images\"\n",
    "    labels_dir = images_dir.replace(\"images\", \"labels\")\n",
    "\n",
    "    # Collect all image paths\n",
    "    img_paths = glob.glob(os.path.join(images_dir, \"*.*\"))\n",
    "\n",
    "    skipped_count = 0\n",
    "\n",
    "    for img_path in tqdm(img_paths, desc=f\"Processing {split_name}\"):\n",
    "        basename = os.path.basename(img_path)       \n",
    "        filebase, _ = os.path.splitext(basename)    \n",
    "        label_path = os.path.join(labels_dir, filebase + \".txt\")\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Label file not found for: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Check if this image was already cropped (skip if yes)\n",
    "        pattern = os.path.join(out_dir, f\"{filebase}_*.jpg\")\n",
    "        existing_crops = glob.glob(pattern)\n",
    "        if existing_crops:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        # Read the image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue  # corrupted or unreadable\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            if not lines:\n",
    "                print(f\"Label file is empty: {label_path}\")\n",
    "                continue\n",
    "            class_id, x_min, y_min, x_max, y_max = yolo_to_xyxy(line, w, h)\n",
    "            crop = image[y_min:y_max, x_min:x_max]\n",
    "            if crop.size == 0:\n",
    "                continue  # empty crop, skip\n",
    "\n",
    "            unique_id = str(uuid.uuid4())[:8]\n",
    "            out_filename = f\"{filebase}_{unique_id}.jpg\"\n",
    "            out_path = os.path.join(out_dir, out_filename)\n",
    "\n",
    "            cv2.imwrite(out_path, crop)\n",
    "            csv_writer.writerow([out_filename, class_id])\n",
    "\n",
    "    print(f\"Done processing {split_name}. Skipped {skipped_count} images.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    yaml_path = os.path.join(base_path, \"YOLO_format/data.yaml\")\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        yaml_dict = yaml.safe_load(f)\n",
    "\n",
    "    # Replace ${HOME} with the actual home directory if present\n",
    "    for key, value in yaml_dict.items():\n",
    "        if isinstance(value, str) and \"${HOME}\" in value:\n",
    "            yaml_dict[key] = value.replace(\"${HOME}\", os.path.expanduser(\"~\"))\n",
    "\n",
    "    base_out = \"cropped_mixed\"\n",
    "    os.makedirs(base_out, exist_ok=True)\n",
    "\n",
    "    for split_name in [\"train\", \"val\", \"test\"]:\n",
    "        out_dir = os.path.join(base_out, split_name)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        csv_path = os.path.join(out_dir, f\"{split_name}_labels.csv\")\n",
    "        with open(csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"filename\", \"class_id\"])\n",
    "            process_split(yaml_dict, split_name, out_dir, writer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# facenet-pytorch\n",
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, images_dir, csv_path, transform=None):\n",
    "        \"\"\"\n",
    "        images_dir: folder containing cropped face images\n",
    "        csv_path: CSV file with 'filename,class_id'\n",
    "        transform: torchvision transforms to apply\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        with open(csv_path, 'r') as f:\n",
    "            reader = csv.DictReader(f)  # handles \"filename,class_id\" header automatically\n",
    "            for row in reader:\n",
    "                fn = row['filename']\n",
    "                label = int(row['class_id'])\n",
    "                self.samples.append((fn, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, label = self.samples[idx]\n",
    "        img_path = os.path.join(self.images_dir, filename)\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceNetClassifier(nn.Module):\n",
    "    def __init__(self, num_emotions=8, freeze_facenet=True):\n",
    "        super(FaceNetClassifier, self).__init__()\n",
    "        # 1) Load FaceNet\n",
    "        self.facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "        \n",
    "        if freeze_facenet:\n",
    "            for param in self.facenet.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # 2) Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_emotions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If facenet is in eval mode and freeze_facenet=True, it uses no_grad().\n",
    "        # For partial fine-tuning, we set freeze_facenet=False & call train() on it \n",
    "        # or just manually set requires_grad to True for certain layers.\n",
    "        with torch.no_grad() if not self.facenet.training else torch.enable_grad():\n",
    "            embeddings = self.facenet(x)  # shape: (batch_size, 512)\n",
    "        out = self.classifier(embeddings)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, device, epochs=5):\n",
    "    \"\"\"\n",
    "    Enhanced training loop using CosineAnnealingLR:\n",
    "      - Tracks train/val loss & accuracy each epoch\n",
    "      - Saves best model weights based on val_acc\n",
    "      - Returns model and metric lists for plotting\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # LR Scheduler: CosineAnnealingLR over 'epochs' cycles\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] started...\")\n",
    "        \n",
    "        # -------- TRAIN MODE --------\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in tqdm(enumerate(train_loader),\n",
    "                                               total=len(train_loader),\n",
    "                                               desc=f\"Training Epoch {epoch+1}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = total_loss / total\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # -------- EVAL MODE --------\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss_total += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_loss = val_loss_total / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Step the LR scheduler\n",
    "        scheduler.step()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "            print(\"Current LR:\", current_lr)\n",
    "\n",
    "        # Save the best model if val_acc improved\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    print(f\"Best val_acc across all epochs: {best_acc:.4f}\")\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Return model + metric lists\n",
    "    return model, (train_losses, train_accuracies, val_losses, val_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 17101\n",
      "Using device: cpu\n",
      "CUDA Available: False\n",
      "Starting training...\n",
      "Epoch [1/15] started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  28%|██▊       | 76/268 [09:27<23:53,  7.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel saved as facenet_emotion_classifier.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     main()\n",
      "\u001b[1;32m/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# 5. Train\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting training...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m trained_model, metrics \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     model, criterion, optimizer, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     train_loader, val_loader, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     device, epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining completed!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m train_losses, train_accuracies, val_losses, val_accuracies \u001b[39m=\u001b[39m metrics\n",
      "\u001b[1;32m/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexhofman/wspace/AI/emotion_recognition/ER_FaceNet.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CNN_env/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CNN_env/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1. Paths to your dataset\n",
    "    train_dir = \"cropped_mixed/train\"\n",
    "    train_csv = os.path.join(train_dir, \"train_labels.csv\")\n",
    "    val_dir = \"cropped_mixed/val\"\n",
    "    val_csv = os.path.join(val_dir, \"val_labels.csv\")\n",
    "\n",
    "    # 2. Augmentations\n",
    "    #    Heavier train augmentation, lighter val transform\n",
    "    train_transform = transforms.Compose([\n",
    "        # Example: random crop approach\n",
    "        transforms.Resize((180, 180)),       # make it bigger first\n",
    "        transforms.RandomCrop((160, 160)),   # then random crop to 160x160\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        # Random Erasing - can help generalize\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.1))\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 3. Datasets & DataLoaders\n",
    "    train_dataset = FaceDataset(train_dir, train_csv, transform=train_transform)\n",
    "    val_dataset = FaceDataset(val_dir, val_csv, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"Total training samples: {len(train_loader.dataset)}\")\n",
    "\n",
    "    # 4. Initialize model, loss, optimizer, device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    if device == \"cuda\":\n",
    "        print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    # freeze_facenet=False to allow partial or full fine-tuning\n",
    "    model = FaceNetClassifier(num_emotions=8, freeze_facenet=False)\n",
    "\n",
    "    # Freeze all layers except the last few\n",
    "    for name, param in model.facenet.named_parameters():\n",
    "        # if \"last_bn\" in name or \"block7\" in name or \"block8\" in name:\n",
    "        #     param.requires_grad = True\n",
    "        # else:\n",
    "        #     param.requires_grad = False\n",
    "        param.requires_grad = True\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=1e-4\n",
    "    )\n",
    "\n",
    "    # 5. Train\n",
    "    print(\"Starting training...\")\n",
    "    trained_model, metrics = train_model(\n",
    "        model, criterion, optimizer, \n",
    "        train_loader, val_loader, \n",
    "        device, epochs=15\n",
    "    )\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    train_losses, train_accuracies, val_losses, val_accuracies = metrics\n",
    "\n",
    "    # Example plotting with matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    epochs_range = range(1, len(train_losses)+1)\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(epochs_range, train_accuracies, label='Train Acc')\n",
    "    plt.plot(epochs_range, val_accuracies, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Train vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # 6. Save your best model\n",
    "    torch.save(trained_model.state_dict(), \"facenet_emotion_classifier.pth\")\n",
    "    print(\"Model saved as facenet_emotion_classifier.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when freezing the last couple of layers (block7, block8, and last_bn) i was getting 69% (it was always less then 70%) and when i unfreeze all i started getting 75.4% and considering i have 17k images to train and strong data augmentation, unfreezing all of the pre trained model made more sense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
