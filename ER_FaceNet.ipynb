{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.12).\n",
      "Path to dataset files: C:\\Users\\user\\.cache\\kagglehub\\datasets\\fatihkgg\\affectnet-yolo-format\\versions\\2\n",
      "Removed the original data.yaml from the Kaggle dataset.\n",
      "Replaced with local data.yaml from the emotion_recognition folder.\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Download the dataset\n",
    "try:\n",
    "    base_path = kagglehub.dataset_download(\"fatihkgg/affectnet-yolo-format\")\n",
    "    print(\"Path to dataset files:\", base_path)\n",
    "except Exception as e:\n",
    "    print(\"Error downloading dataset:\", e)\n",
    "    exit()\n",
    "\n",
    "# Path to the Kaggle dataset's data.yaml\n",
    "kaggle_data_yaml_path = os.path.join(base_path, \"YOLO_format/data.yaml\")\n",
    "\n",
    "# If the file already exists from the downloaded dataset, remove it\n",
    "if os.path.exists(kaggle_data_yaml_path):\n",
    "    os.remove(kaggle_data_yaml_path)\n",
    "    print(\"Removed the original data.yaml from the Kaggle dataset.\")\n",
    "\n",
    "# Path to your custom data.yaml in the emotion_recognition folder\n",
    "local_data_yaml_path = \"./data.yaml\"\n",
    "\n",
    "# Copy your custom data.yaml to the dataset folder\n",
    "shutil.copy(local_data_yaml_path, kaggle_data_yaml_path)\n",
    "print(\"Replaced with local data.yaml from the emotion_recognition folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 17101/17101 [00:00<00:00, 174592.51it/s]\n",
      "Processing val: 100%|██████████| 5406/5406 [00:00<00:00, 161591.85it/s]\n",
      "Processing test: 100%|██████████| 2755/2755 [00:00<00:00, 152410.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import csv\n",
    "import uuid  # for unique filenames\n",
    "\n",
    "def get_image_base(filename):\n",
    "    \"\"\"\n",
    "    Given a filename like \"ffhq_2.jpg\" or \"ffhq_2_cropped.jpg\" or \"image0000020.png\",\n",
    "    return the \"base\" the way the user wants it:\n",
    "      - If it starts with \"ffhq\", then join the first two parts: \"ffhq_2\"\n",
    "      - Otherwise, just take the first part: \"image0000020\"\n",
    "    \"\"\"\n",
    "    name, _ = os.path.splitext(filename)      # remove extension\n",
    "    parts = name.split('_')\n",
    "    if len(parts) > 1 and parts[0] == 'ffhq':\n",
    "        return '_'.join(parts[:2])  # e.g. \"ffhq_2\"\n",
    "    else:\n",
    "        return parts[0]            # e.g. \"image0000020\"\n",
    "\n",
    "def yolo_to_xyxy(label_line, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert YOLO label line (class_id, x_center, y_center, width, height)\n",
    "    from normalized coords [0-1] to absolute pixel coords [x_min, y_min, x_max, y_max].\n",
    "    \"\"\"\n",
    "    parts = label_line.strip().split()\n",
    "    class_id = int(parts[0])\n",
    "    x_center = float(parts[1]) * img_width\n",
    "    y_center = float(parts[2]) * img_height\n",
    "    w = float(parts[3]) * img_width\n",
    "    h = float(parts[4]) * img_height\n",
    "\n",
    "    x_min = int(x_center - w / 2)\n",
    "    y_min = int(y_center - h / 2)\n",
    "    x_max = int(x_center + w / 2)\n",
    "    y_max = int(y_center + h / 2)\n",
    "\n",
    "    # Clip to image boundaries\n",
    "    x_min = max(0, x_min)\n",
    "    y_min = max(0, y_min)\n",
    "    x_max = min(img_width, x_max)\n",
    "    y_max = min(img_height, y_max)\n",
    "\n",
    "    return class_id, x_min, y_min, x_max, y_max\n",
    "\n",
    "\n",
    "def process_split(yaml_dict, split_name, out_dir, csv_writer):\n",
    "    \"\"\"\n",
    "    Process one split (train, valid, or test).\n",
    "    - yaml_dict: loaded from data.yaml\n",
    "    - split_name: 'train', 'val', or 'test'\n",
    "    - out_dir: where to store all cropped images for this split (still \"mixed\")\n",
    "    - csv_writer: CSV writer to record (filename, class_id) rows\n",
    "    \"\"\"\n",
    "\n",
    "    # e.g. for train:\n",
    "    images_dir = yaml_dict[split_name]         # \"/kaggle/.../train/images\"\n",
    "    labels_dir = images_dir.replace(\"images\", \"labels\")\n",
    "\n",
    "    # 1) First, detect which images have already been processed in out_dir\n",
    "    #    so we can skip them.\n",
    "    existing_crops = os.listdir(out_dir)\n",
    "    processed_bases = set()\n",
    "    for cropped_file in existing_crops:\n",
    "        base = get_image_base(cropped_file)\n",
    "        processed_bases.add(base)\n",
    "\n",
    "    # 2) Collect all image paths in the main dataset\n",
    "    img_paths = glob.glob(os.path.join(images_dir, \"*.*\"))  # jpg, png, etc.\n",
    "\n",
    "    for img_path in tqdm(img_paths, desc=f\"Processing {split_name}\"):\n",
    "        basename = os.path.basename(img_path)        # e.g. \"image_12.png\"\n",
    "        filebase, _ = os.path.splitext(basename)     # e.g. \"image_12\"\n",
    "\n",
    "        # Figure out the \"base\" for this image.\n",
    "        main_base = get_image_base(basename)\n",
    "        if main_base in processed_bases:\n",
    "            # We've already done this image. Skip.\n",
    "            continue\n",
    "\n",
    "        label_path = os.path.join(labels_dir, filebase + \".txt\")\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Label file not found for: {img_path}\")\n",
    "            continue  # no label, skip\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue  # corrupted or unreadable\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        if not lines:\n",
    "            print(f\"Label file is empty: {label_path}\")\n",
    "            continue\n",
    "\n",
    "        # 3) Process each bounding box\n",
    "        for line in lines:\n",
    "            class_id, x_min, y_min, x_max, y_max = yolo_to_xyxy(line, w, h)\n",
    "            crop = image[y_min:y_max, x_min:x_max]\n",
    "            if crop.size == 0:\n",
    "                continue  # empty crop, skip\n",
    "\n",
    "            # Generate a unique filename so we don't overwrite\n",
    "            unique_id = str(uuid.uuid4())[:8]  \n",
    "            out_filename = f\"{filebase}_{unique_id}.jpg\"\n",
    "            out_path = os.path.join(out_dir, out_filename)\n",
    "\n",
    "            # Save the cropped face\n",
    "            cv2.imwrite(out_path, crop)\n",
    "\n",
    "            # Write a row in the CSV: [cropped_filename, class_id]\n",
    "            csv_writer.writerow([out_filename, class_id])\n",
    "\n",
    "        # 4) Since we've now processed this image, add its base to the set\n",
    "        processed_bases.add(main_base)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage: \n",
    "    1) Reads data.yaml\n",
    "    2) Processes train, val, test\n",
    "    3) Saves all cropped faces in separate 'cropped_mixed/train', 'cropped_mixed/val', 'cropped_mixed/test' dirs\n",
    "    4) For each split, writes a CSV file with 'filename,class_id'\n",
    "    \"\"\"\n",
    "    # Path to your data.yaml\n",
    "    yaml_path = os.path.join(base_path, \"YOLO_format/data.yaml\")\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        yaml_dict = yaml.safe_load(f)\n",
    "\n",
    "    # Replace ${HOME} with the actual home directory\n",
    "    for key, value in yaml_dict.items():\n",
    "        if isinstance(value, str) and \"${HOME}\" in value:\n",
    "            yaml_dict[key] = value.replace(\"${HOME}\", os.path.expanduser(\"~\"))\n",
    "\n",
    "    # Create a base output folder\n",
    "    base_out = \"cropped_mixed\"\n",
    "    os.makedirs(base_out, exist_ok=True)\n",
    "\n",
    "    # We'll process each split separately, but keep them \"mixed\" (no subfolders by class)\n",
    "    for split_name in [\"train\", \"val\", \"test\"]:\n",
    "        out_dir = os.path.join(base_out, split_name)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        csv_path = os.path.join(out_dir, f\"{split_name}_labels.csv\")\n",
    "\n",
    "        # 1) If there's already a CSV, read its contents so we don't lose them\n",
    "        existing_rows = []\n",
    "        if os.path.exists(csv_path):\n",
    "            with open(csv_path, 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                existing_rows = list(reader)  # old header + data rows\n",
    "\n",
    "        # 2) Open the same file in 'w' mode, so we can rewrite old + new\n",
    "        with open(csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "\n",
    "            # Write your header again\n",
    "            writer.writerow([\"filename\", \"class_id\"])\n",
    "\n",
    "            # 3) Re-write the old rows (skipping the old header if needed)\n",
    "            for row in existing_rows[1:]:\n",
    "                writer.writerow(row)\n",
    "\n",
    "            # 4) Call process_split to add new (cropped) rows\n",
    "            process_split(yaml_dict, split_name, out_dir, writer)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import warnings\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# facenet-pytorch\n",
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, images_dir, csv_path, transform=None):\n",
    "        \"\"\"\n",
    "        images_dir: folder containing cropped face images\n",
    "        csv_path: CSV file with 'filename,class_id'\n",
    "        transform: torchvision transforms to apply\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        with open(csv_path, 'r') as f:\n",
    "            reader = csv.DictReader(f)  # handles \"filename,class_id\" header automatically\n",
    "            for row in reader:\n",
    "                fn = row['filename']\n",
    "                label = int(row['class_id'])\n",
    "                self.samples.append((fn, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, label = self.samples[idx]\n",
    "        img_path = os.path.join(self.images_dir, filename)\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceNetClassifier(nn.Module):\n",
    "    def __init__(self, num_emotions=8, freeze_facenet=True):\n",
    "        super(FaceNetClassifier, self).__init__()\n",
    "        # 1) Load FaceNet\n",
    "        self.facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "        \n",
    "        if freeze_facenet:\n",
    "            for param in self.facenet.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # 2) Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),    # first dense\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),    # second dense\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_emotions)  # final logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If facenet is in eval mode and freeze_facenet=True, it uses no_grad().\n",
    "        # For partial fine-tuning, we set freeze_facenet=False & call train() on it \n",
    "        # or just manually set requires_grad to True for certain layers.\n",
    "        with torch.no_grad() if not self.facenet.training else torch.enable_grad():\n",
    "            embeddings = self.facenet(x)  # shape: (batch_size, 512)\n",
    "        out = self.classifier(embeddings)\n",
    "        return out\n",
    "    \n",
    "    def model_summary(self):\n",
    "        print(\"FaceNetClassifier model:\")\n",
    "        print(\"  Feature extractor: InceptionResnetV1\")\n",
    "        print(\"  Classifier head:\")\n",
    "        # Convert the classifier to a string and indent it with 4 spaces\n",
    "        classifier_str = textwrap.indent(str(self.classifier), \"    \")\n",
    "        print(classifier_str)\n",
    "\n",
    "    def test_model(self, test_loader, criterion, device):\n",
    "        self.eval()  # Set the entire model to evaluation mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Turn off gradient calculation for inference\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                # Move data to the appropriate device (CPU or GPU)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass using the whole model (feature extractor + classifier)\n",
    "                outputs = self(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * images.size(0)  # Sum loss over the batch\n",
    "                \n",
    "                # Compute predictions and accumulate correct predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate average loss and accuracy\n",
    "        test_loss = running_loss / total\n",
    "        test_acc = correct / total\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FaceNetClassifier model:\n",
      "  Feature extractor: InceptionResnetV1\n",
      "  Classifier head:\n",
      "    Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=128, out_features=8, bias=True)\n",
      "    )\n",
      "Test accuracy: 0.169 ... (⌒_⌒;)\n"
     ]
    }
   ],
   "source": [
    "from cgi import test\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "model = FaceNetClassifier()\n",
    "model.model_summary()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# And a device:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move your model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate the model\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "test_dir = \"cropped_mixed/test\"\n",
    "test_csv = os.path.join(test_dir, \"test_labels.csv\")\n",
    "test_dataset = FaceDataset(test_dir, test_csv, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "test_loss, test_acc = model.test_model(test_loader, criterion, device)\n",
    "print(f\"Test accuracy: {test_acc:.3f} ... (⌒_⌒;)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "def train_model(model, criterion, optimizer, \n",
    "                train_loader, val_loader, \n",
    "                device, class_names=None,\n",
    "                epochs=5):\n",
    "    \"\"\"\n",
    "    Training loop with:\n",
    "      - CosineAnnealingLR scheduler\n",
    "      - Tracking of loss, accuracy, precision, recall, F1 per epoch\n",
    "      - Saving best‐val_acc weights + best confusion matrix\n",
    "    Returns:\n",
    "      - best‐model\n",
    "      - history dict containing lists of all metrics\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # To save best weights + confusion matrix\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_conf_matrix = None\n",
    "\n",
    "    # History containers\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [],   'val_acc': [],\n",
    "        'val_prec': [],   'val_rec': [], 'val_f1': [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n",
    "\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=\"  Train\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss    += loss.item() * images.size(0)\n",
    "            _, preds         = torch.max(outputs, 1)\n",
    "            running_corrects+= (preds == labels).sum().item()\n",
    "            total_samples   += labels.size(0)\n",
    "\n",
    "        epoch_train_loss = running_loss / total_samples\n",
    "        epoch_train_acc  = running_corrects / total_samples\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        model.eval()\n",
    "        val_loss    = 0.0\n",
    "        val_correct = 0\n",
    "        val_total   = 0\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=\"  Val\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss    = criterion(outputs, labels)\n",
    "\n",
    "                val_loss    += loss.item() * images.size(0)\n",
    "                _, preds     = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total   += labels.size(0)\n",
    "\n",
    "                y_true.extend(labels.cpu().tolist())\n",
    "                y_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "        epoch_val_loss = val_loss / val_total\n",
    "        epoch_val_acc  = val_correct / val_total\n",
    "\n",
    "        # compute precision/recall/f1\n",
    "        epoch_prec = precision_score(y_true, y_pred, \n",
    "                                     average='weighted', zero_division=0)\n",
    "        epoch_rec  = recall_score   (y_true, y_pred, \n",
    "                                     average='weighted', zero_division=0)\n",
    "        epoch_f1   = f1_score       (y_true, y_pred, \n",
    "                                     average='weighted', zero_division=0)\n",
    "        cm         = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "        history['val_prec'].append(epoch_prec)\n",
    "        history['val_rec'].append(epoch_rec)\n",
    "        history['val_f1'].append(epoch_f1)\n",
    "\n",
    "        # step scheduler & print LR\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(f\"  Train — loss: {epoch_train_loss:.4f}, acc: {epoch_train_acc:.4f}\")\n",
    "        print(f\"  Val   — loss: {epoch_val_loss:.4f}, acc: {epoch_val_acc:.4f}, \"\n",
    "              f\"prec: {epoch_prec:.4f}, rec: {epoch_rec:.4f}, f1: {epoch_f1:.4f}\")\n",
    "        print(f\"  LR: {current_lr:.6f}\")\n",
    "\n",
    "        # save best\n",
    "        if epoch_val_acc > best_acc:\n",
    "            best_acc = epoch_val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_conf_matrix = cm.copy()\n",
    "\n",
    "    print(f\"\\nBest Val Acc: {best_acc:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Package history + best metrics\n",
    "    history['best_val_acc'] = best_acc\n",
    "    history['best_conf_matrix'] = best_conf_matrix\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FaceDataset' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 32\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m val_loader   \u001b[38;5;241m=\u001b[39m DataLoader(val_ds,   batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# grab class names for confusion matrix plotting\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m class_names \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m  \u001b[38;5;66;03m# or however your dataset exposes them\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 4. Model / loss / optimizer / device\u001b[39;00m\n\u001b[0;32m     35\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FaceDataset' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1. Paths to your dataset\n",
    "    train_dir = \"cropped_mixed/train\"\n",
    "    train_csv = os.path.join(train_dir, \"train_labels.csv\")\n",
    "    val_dir   = \"cropped_mixed/val\"\n",
    "    val_csv   = os.path.join(val_dir,   \"val_labels.csv\")\n",
    "\n",
    "    # 2. Augmentations\n",
    "    train_transform = transforms.Compose([\n",
    "        # Example: random crop approach\n",
    "        transforms.Resize((180, 180)),       # make it bigger first\n",
    "        transforms.RandomCrop((160, 160)),   # then random crop to 160x160\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "        transforms.ToTensor(),\n",
    "        # Random Erasing - can help generalize\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.1))\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 3. Datasets & DataLoaders\n",
    "    train_ds = FaceDataset(train_dir, train_csv, transform=train_transform)\n",
    "    val_ds   = FaceDataset(val_dir,   val_csv,   transform=val_transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    # grab class names for confusion matrix plotting\n",
    "    class_names = train_ds.classes  # or however your dataset exposes them\n",
    "\n",
    "    # 4. Model / loss / optimizer / device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    model = FaceNetClassifier(num_emotions=len(class_names), freeze_facenet=False)\n",
    "    # unfreeze everything (or tweak per-layer)\n",
    "    for p in model.facenet.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "    # 5. Train\n",
    "    print(\"Starting training...\")\n",
    "    trained_model, history = train_model(\n",
    "        model, criterion, optimizer,\n",
    "        train_loader, val_loader,\n",
    "        device, class_names=class_names,\n",
    "        epochs=15\n",
    "    )\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    # 6. Extract metrics from history\n",
    "    train_losses = history['train_loss']\n",
    "    train_accs   = history['train_acc']\n",
    "    val_losses   = history['val_loss']\n",
    "    val_accs     = history['val_acc']\n",
    "    val_precs    = history['val_prec']\n",
    "    val_recs     = history['val_rec']\n",
    "    val_f1s      = history['val_f1']\n",
    "    best_acc     = history['best_val_acc']\n",
    "    best_cm      = history['best_conf_matrix']\n",
    "\n",
    "    # 7. Plot training curves\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses,   label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "    plt.title('Train vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(epochs, train_accs, label='Train Acc')\n",
    "    plt.plot(epochs, val_accs,   label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Train vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(epochs, val_precs, label='Val Precision')\n",
    "    plt.plot(epochs, val_recs,  label='Val Recall')\n",
    "    plt.plot(epochs, val_f1s,    label='Val F1-score')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Score')\n",
    "    plt.title('Validation Precision / Recall / F1')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # 8. Plot best confusion matrix\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(best_cm, annot=True, fmt='d',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names,\n",
    "                cbar=False)\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix @ Best Val Acc ({best_acc:.3f})')\n",
    "    plt.show()\n",
    "\n",
    "    # 9. Save best model\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    fn = f\"models/facenet_emotion_classifier_{best_acc:.3f}.pth\"\n",
    "    torch.save(trained_model.state_dict(), fn)\n",
    "    print(f\"Model saved to {fn}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when freezing the last couple of layers (block7, block8, and last_bn) i was getting 69% (it was always less then 70%) and when i unfreeze all i started getting 75.4% and considering i have 17k images to train and strong data augmentation, unfreezing all of the pre trained model made more sense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
