{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.8).\n",
      "Path to dataset files: C:\\Users\\user\\.cache\\kagglehub\\datasets\\fatihkgg\\affectnet-yolo-format\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download the dataset\n",
    "try:\n",
    "    base_path = kagglehub.dataset_download(\"fatihkgg/affectnet-yolo-format\")\n",
    "    print(\"Path to dataset files:\", base_path)\n",
    "except Exception as e:\n",
    "    print(\"Error downloading dataset:\", e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 17101/17101 [00:29<00:00, 581.55it/s]\n",
      "Processing val: 100%|██████████| 5406/5406 [00:08<00:00, 627.28it/s]\n",
      "Processing test: 100%|██████████| 2755/2755 [00:04<00:00, 638.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import csv\n",
    "import uuid  # for unique filenames\n",
    "\n",
    "def yolo_to_xyxy(label_line, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert YOLO label line (class_id, x_center, y_center, width, height)\n",
    "    from normalized coords [0-1] to absolute pixel coords [x_min, y_min, x_max, y_max].\n",
    "    \"\"\"\n",
    "    parts = label_line.strip().split()\n",
    "    class_id = int(parts[0])\n",
    "    x_center = float(parts[1]) * img_width\n",
    "    y_center = float(parts[2]) * img_height\n",
    "    w = float(parts[3]) * img_width\n",
    "    h = float(parts[4]) * img_height\n",
    "\n",
    "    x_min = int(x_center - w / 2)\n",
    "    y_min = int(y_center - h / 2)\n",
    "    x_max = int(x_center + w / 2)\n",
    "    y_max = int(y_center + h / 2)\n",
    "\n",
    "    # Clip to image boundaries\n",
    "    x_min = max(0, x_min)\n",
    "    y_min = max(0, y_min)\n",
    "    x_max = min(img_width, x_max)\n",
    "    y_max = min(img_height, y_max)\n",
    "\n",
    "    return class_id, x_min, y_min, x_max, y_max\n",
    "\n",
    "\n",
    "def process_split(yaml_dict, split_name, out_dir, csv_writer):\n",
    "    \"\"\"\n",
    "    Process one split (train, valid, or test).\n",
    "    - yaml_dict: loaded from data.yaml\n",
    "    - split_name: 'train', 'val', or 'test'\n",
    "    - out_dir: where to store all cropped images for this split (still \"mixed\")\n",
    "    - csv_writer: CSV writer to record (filename, class_id) rows\n",
    "    \"\"\"\n",
    "\n",
    "    # e.g. for train:\n",
    "    images_dir = yaml_dict[split_name]         # \"/kaggle/../train/images\"\n",
    "    labels_dir = images_dir.replace(\"images\", \"labels\")\n",
    "\n",
    "    # Collect all image paths\n",
    "    img_paths = glob.glob(os.path.join(images_dir, \"*.*\"))  # jpg, png, etc.\n",
    "\n",
    "    for img_path in tqdm(img_paths, desc=f\"Processing {split_name}\"):\n",
    "        basename = os.path.basename(img_path)        # e.g. \"image_12.png\"\n",
    "        filebase, _ = os.path.splitext(basename)     # e.g. \"image_12\"\n",
    "        label_path = os.path.join(labels_dir, filebase + \".txt\")\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Label file not found for: {img_path}\")\n",
    "            continue  # no label, skip\n",
    "\n",
    "        # Read the image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue  # corrupted or unreadable\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Each line may represent a different bounding box\n",
    "        for line in lines:\n",
    "            if not lines:\n",
    "                print(f\"Label file is empty: {label_path}\")\n",
    "                continue  # empty label file\n",
    "            class_id, x_min, y_min, x_max, y_max = yolo_to_xyxy(line, w, h)\n",
    "            crop = image[y_min:y_max, x_min:x_max]\n",
    "            if crop.size == 0:\n",
    "                continue  # empty crop, skip\n",
    "\n",
    "            # Generate a unique filename so we don't overwrite\n",
    "            unique_id = str(uuid.uuid4())[:8]  \n",
    "            out_filename = f\"{filebase}_{unique_id}.jpg\"\n",
    "            out_path = os.path.join(out_dir, out_filename)\n",
    "\n",
    "            # Save the cropped face\n",
    "            cv2.imwrite(out_path, crop)\n",
    "\n",
    "            # Write a row in the CSV: [cropped_filename, class_id]\n",
    "            csv_writer.writerow([out_filename, class_id])\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage: \n",
    "    1) Reads data.yaml\n",
    "    2) Processes train, val, test\n",
    "    3) Saves all cropped faces in separate 'cropped_mixed/train', 'cropped_mixed/val', 'cropped_mixed/test' dirs\n",
    "    4) For each split, writes a CSV file with 'filename,class_id'\n",
    "    \"\"\"\n",
    "    # Path to your data.yaml\n",
    "    yaml_path = os.path.join(base_path, \"YOLO_format/data.yaml\")\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        yaml_dict = yaml.safe_load(f)\n",
    "\n",
    "    # Create a base output folder\n",
    "    base_out = \"cropped_mixed\"\n",
    "    os.makedirs(base_out, exist_ok=True)\n",
    "\n",
    "    # We'll process each split separately, but keep them \"mixed\" (no subfolders by class)\n",
    "    for split_name in [\"train\", \"val\", \"test\"]:\n",
    "        out_dir = os.path.join(base_out, split_name)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        # We'll store label info in a CSV for each split\n",
    "        csv_path = os.path.join(out_dir, f\"{split_name}_labels.csv\")\n",
    "        with open(csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            # Write a header row if you like\n",
    "            writer.writerow([\"filename\", \"class_id\"])\n",
    "            \n",
    "            process_split(yaml_dict, split_name, out_dir, writer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# facenet-pytorch\n",
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, images_dir, csv_path, transform=None):\n",
    "        \"\"\"\n",
    "        images_dir: folder containing cropped face images\n",
    "        csv_path: CSV file with 'filename,class_id'\n",
    "        transform: torchvision transforms to apply\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        with open(csv_path, 'r') as f:\n",
    "            reader = csv.DictReader(f)  # handles \"filename,class_id\" header automatically\n",
    "            for row in reader:\n",
    "                fn = row['filename']\n",
    "                label = int(row['class_id'])\n",
    "                self.samples.append((fn, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, label = self.samples[idx]\n",
    "        img_path = os.path.join(self.images_dir, filename)\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceNetClassifier(nn.Module):\n",
    "    def __init__(self, num_emotions=8, freeze_facenet=True):\n",
    "        super(FaceNetClassifier, self).__init__()\n",
    "        # 1) Load FaceNet\n",
    "        self.facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "        \n",
    "        if freeze_facenet:\n",
    "            for param in self.facenet.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # 2) Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_emotions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If facenet is in eval mode and freeze_facenet=True, it uses no_grad().\n",
    "        # For partial fine-tuning, we set freeze_facenet=False & call train() on it \n",
    "        # or just manually set requires_grad to True for certain layers.\n",
    "        with torch.no_grad() if not self.facenet.training else torch.enable_grad():\n",
    "            embeddings = self.facenet(x)  # shape: (batch_size, 512)\n",
    "        out = self.classifier(embeddings)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, device, epochs=5):\n",
    "    \"\"\"\n",
    "    Enhanced training loop using CosineAnnealingLR:\n",
    "      - Tracks train/val loss & accuracy each epoch\n",
    "      - Saves best model weights based on val_acc\n",
    "      - Returns model and metric lists for plotting\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # LR Scheduler: CosineAnnealingLR over 'epochs' cycles\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] started...\")\n",
    "        \n",
    "        # -------- TRAIN MODE --------\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in tqdm(enumerate(train_loader),\n",
    "                                               total=len(train_loader),\n",
    "                                               desc=f\"Training Epoch {epoch+1}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = total_loss / total\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # -------- EVAL MODE --------\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss_total += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_loss = val_loss_total / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Step the LR scheduler\n",
    "        scheduler.step()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "            print(\"Current LR:\", current_lr)\n",
    "\n",
    "        # Save the best model if val_acc improved\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    print(f\"Best val_acc across all epochs: {best_acc:.4f}\")\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Return model + metric lists\n",
    "    return model, (train_losses, train_accuracies, val_losses, val_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 17101\n",
      "Using device: cuda\n",
      "CUDA Available: True\n",
      "conv2d_1a.conv.weight\n",
      "conv2d_1a.bn.weight\n",
      "conv2d_1a.bn.bias\n",
      "conv2d_2a.conv.weight\n",
      "conv2d_2a.bn.weight\n",
      "conv2d_2a.bn.bias\n",
      "conv2d_2b.conv.weight\n",
      "conv2d_2b.bn.weight\n",
      "conv2d_2b.bn.bias\n",
      "conv2d_3b.conv.weight\n",
      "conv2d_3b.bn.weight\n",
      "conv2d_3b.bn.bias\n",
      "conv2d_4a.conv.weight\n",
      "conv2d_4a.bn.weight\n",
      "conv2d_4a.bn.bias\n",
      "conv2d_4b.conv.weight\n",
      "conv2d_4b.bn.weight\n",
      "conv2d_4b.bn.bias\n",
      "repeat_1.0.branch0.conv.weight\n",
      "repeat_1.0.branch0.bn.weight\n",
      "repeat_1.0.branch0.bn.bias\n",
      "repeat_1.0.branch1.0.conv.weight\n",
      "repeat_1.0.branch1.0.bn.weight\n",
      "repeat_1.0.branch1.0.bn.bias\n",
      "repeat_1.0.branch1.1.conv.weight\n",
      "repeat_1.0.branch1.1.bn.weight\n",
      "repeat_1.0.branch1.1.bn.bias\n",
      "repeat_1.0.branch2.0.conv.weight\n",
      "repeat_1.0.branch2.0.bn.weight\n",
      "repeat_1.0.branch2.0.bn.bias\n",
      "repeat_1.0.branch2.1.conv.weight\n",
      "repeat_1.0.branch2.1.bn.weight\n",
      "repeat_1.0.branch2.1.bn.bias\n",
      "repeat_1.0.branch2.2.conv.weight\n",
      "repeat_1.0.branch2.2.bn.weight\n",
      "repeat_1.0.branch2.2.bn.bias\n",
      "repeat_1.0.conv2d.weight\n",
      "repeat_1.0.conv2d.bias\n",
      "repeat_1.1.branch0.conv.weight\n",
      "repeat_1.1.branch0.bn.weight\n",
      "repeat_1.1.branch0.bn.bias\n",
      "repeat_1.1.branch1.0.conv.weight\n",
      "repeat_1.1.branch1.0.bn.weight\n",
      "repeat_1.1.branch1.0.bn.bias\n",
      "repeat_1.1.branch1.1.conv.weight\n",
      "repeat_1.1.branch1.1.bn.weight\n",
      "repeat_1.1.branch1.1.bn.bias\n",
      "repeat_1.1.branch2.0.conv.weight\n",
      "repeat_1.1.branch2.0.bn.weight\n",
      "repeat_1.1.branch2.0.bn.bias\n",
      "repeat_1.1.branch2.1.conv.weight\n",
      "repeat_1.1.branch2.1.bn.weight\n",
      "repeat_1.1.branch2.1.bn.bias\n",
      "repeat_1.1.branch2.2.conv.weight\n",
      "repeat_1.1.branch2.2.bn.weight\n",
      "repeat_1.1.branch2.2.bn.bias\n",
      "repeat_1.1.conv2d.weight\n",
      "repeat_1.1.conv2d.bias\n",
      "repeat_1.2.branch0.conv.weight\n",
      "repeat_1.2.branch0.bn.weight\n",
      "repeat_1.2.branch0.bn.bias\n",
      "repeat_1.2.branch1.0.conv.weight\n",
      "repeat_1.2.branch1.0.bn.weight\n",
      "repeat_1.2.branch1.0.bn.bias\n",
      "repeat_1.2.branch1.1.conv.weight\n",
      "repeat_1.2.branch1.1.bn.weight\n",
      "repeat_1.2.branch1.1.bn.bias\n",
      "repeat_1.2.branch2.0.conv.weight\n",
      "repeat_1.2.branch2.0.bn.weight\n",
      "repeat_1.2.branch2.0.bn.bias\n",
      "repeat_1.2.branch2.1.conv.weight\n",
      "repeat_1.2.branch2.1.bn.weight\n",
      "repeat_1.2.branch2.1.bn.bias\n",
      "repeat_1.2.branch2.2.conv.weight\n",
      "repeat_1.2.branch2.2.bn.weight\n",
      "repeat_1.2.branch2.2.bn.bias\n",
      "repeat_1.2.conv2d.weight\n",
      "repeat_1.2.conv2d.bias\n",
      "repeat_1.3.branch0.conv.weight\n",
      "repeat_1.3.branch0.bn.weight\n",
      "repeat_1.3.branch0.bn.bias\n",
      "repeat_1.3.branch1.0.conv.weight\n",
      "repeat_1.3.branch1.0.bn.weight\n",
      "repeat_1.3.branch1.0.bn.bias\n",
      "repeat_1.3.branch1.1.conv.weight\n",
      "repeat_1.3.branch1.1.bn.weight\n",
      "repeat_1.3.branch1.1.bn.bias\n",
      "repeat_1.3.branch2.0.conv.weight\n",
      "repeat_1.3.branch2.0.bn.weight\n",
      "repeat_1.3.branch2.0.bn.bias\n",
      "repeat_1.3.branch2.1.conv.weight\n",
      "repeat_1.3.branch2.1.bn.weight\n",
      "repeat_1.3.branch2.1.bn.bias\n",
      "repeat_1.3.branch2.2.conv.weight\n",
      "repeat_1.3.branch2.2.bn.weight\n",
      "repeat_1.3.branch2.2.bn.bias\n",
      "repeat_1.3.conv2d.weight\n",
      "repeat_1.3.conv2d.bias\n",
      "repeat_1.4.branch0.conv.weight\n",
      "repeat_1.4.branch0.bn.weight\n",
      "repeat_1.4.branch0.bn.bias\n",
      "repeat_1.4.branch1.0.conv.weight\n",
      "repeat_1.4.branch1.0.bn.weight\n",
      "repeat_1.4.branch1.0.bn.bias\n",
      "repeat_1.4.branch1.1.conv.weight\n",
      "repeat_1.4.branch1.1.bn.weight\n",
      "repeat_1.4.branch1.1.bn.bias\n",
      "repeat_1.4.branch2.0.conv.weight\n",
      "repeat_1.4.branch2.0.bn.weight\n",
      "repeat_1.4.branch2.0.bn.bias\n",
      "repeat_1.4.branch2.1.conv.weight\n",
      "repeat_1.4.branch2.1.bn.weight\n",
      "repeat_1.4.branch2.1.bn.bias\n",
      "repeat_1.4.branch2.2.conv.weight\n",
      "repeat_1.4.branch2.2.bn.weight\n",
      "repeat_1.4.branch2.2.bn.bias\n",
      "repeat_1.4.conv2d.weight\n",
      "repeat_1.4.conv2d.bias\n",
      "mixed_6a.branch0.conv.weight\n",
      "mixed_6a.branch0.bn.weight\n",
      "mixed_6a.branch0.bn.bias\n",
      "mixed_6a.branch1.0.conv.weight\n",
      "mixed_6a.branch1.0.bn.weight\n",
      "mixed_6a.branch1.0.bn.bias\n",
      "mixed_6a.branch1.1.conv.weight\n",
      "mixed_6a.branch1.1.bn.weight\n",
      "mixed_6a.branch1.1.bn.bias\n",
      "mixed_6a.branch1.2.conv.weight\n",
      "mixed_6a.branch1.2.bn.weight\n",
      "mixed_6a.branch1.2.bn.bias\n",
      "repeat_2.0.branch0.conv.weight\n",
      "repeat_2.0.branch0.bn.weight\n",
      "repeat_2.0.branch0.bn.bias\n",
      "repeat_2.0.branch1.0.conv.weight\n",
      "repeat_2.0.branch1.0.bn.weight\n",
      "repeat_2.0.branch1.0.bn.bias\n",
      "repeat_2.0.branch1.1.conv.weight\n",
      "repeat_2.0.branch1.1.bn.weight\n",
      "repeat_2.0.branch1.1.bn.bias\n",
      "repeat_2.0.branch1.2.conv.weight\n",
      "repeat_2.0.branch1.2.bn.weight\n",
      "repeat_2.0.branch1.2.bn.bias\n",
      "repeat_2.0.conv2d.weight\n",
      "repeat_2.0.conv2d.bias\n",
      "repeat_2.1.branch0.conv.weight\n",
      "repeat_2.1.branch0.bn.weight\n",
      "repeat_2.1.branch0.bn.bias\n",
      "repeat_2.1.branch1.0.conv.weight\n",
      "repeat_2.1.branch1.0.bn.weight\n",
      "repeat_2.1.branch1.0.bn.bias\n",
      "repeat_2.1.branch1.1.conv.weight\n",
      "repeat_2.1.branch1.1.bn.weight\n",
      "repeat_2.1.branch1.1.bn.bias\n",
      "repeat_2.1.branch1.2.conv.weight\n",
      "repeat_2.1.branch1.2.bn.weight\n",
      "repeat_2.1.branch1.2.bn.bias\n",
      "repeat_2.1.conv2d.weight\n",
      "repeat_2.1.conv2d.bias\n",
      "repeat_2.2.branch0.conv.weight\n",
      "repeat_2.2.branch0.bn.weight\n",
      "repeat_2.2.branch0.bn.bias\n",
      "repeat_2.2.branch1.0.conv.weight\n",
      "repeat_2.2.branch1.0.bn.weight\n",
      "repeat_2.2.branch1.0.bn.bias\n",
      "repeat_2.2.branch1.1.conv.weight\n",
      "repeat_2.2.branch1.1.bn.weight\n",
      "repeat_2.2.branch1.1.bn.bias\n",
      "repeat_2.2.branch1.2.conv.weight\n",
      "repeat_2.2.branch1.2.bn.weight\n",
      "repeat_2.2.branch1.2.bn.bias\n",
      "repeat_2.2.conv2d.weight\n",
      "repeat_2.2.conv2d.bias\n",
      "repeat_2.3.branch0.conv.weight\n",
      "repeat_2.3.branch0.bn.weight\n",
      "repeat_2.3.branch0.bn.bias\n",
      "repeat_2.3.branch1.0.conv.weight\n",
      "repeat_2.3.branch1.0.bn.weight\n",
      "repeat_2.3.branch1.0.bn.bias\n",
      "repeat_2.3.branch1.1.conv.weight\n",
      "repeat_2.3.branch1.1.bn.weight\n",
      "repeat_2.3.branch1.1.bn.bias\n",
      "repeat_2.3.branch1.2.conv.weight\n",
      "repeat_2.3.branch1.2.bn.weight\n",
      "repeat_2.3.branch1.2.bn.bias\n",
      "repeat_2.3.conv2d.weight\n",
      "repeat_2.3.conv2d.bias\n",
      "repeat_2.4.branch0.conv.weight\n",
      "repeat_2.4.branch0.bn.weight\n",
      "repeat_2.4.branch0.bn.bias\n",
      "repeat_2.4.branch1.0.conv.weight\n",
      "repeat_2.4.branch1.0.bn.weight\n",
      "repeat_2.4.branch1.0.bn.bias\n",
      "repeat_2.4.branch1.1.conv.weight\n",
      "repeat_2.4.branch1.1.bn.weight\n",
      "repeat_2.4.branch1.1.bn.bias\n",
      "repeat_2.4.branch1.2.conv.weight\n",
      "repeat_2.4.branch1.2.bn.weight\n",
      "repeat_2.4.branch1.2.bn.bias\n",
      "repeat_2.4.conv2d.weight\n",
      "repeat_2.4.conv2d.bias\n",
      "repeat_2.5.branch0.conv.weight\n",
      "repeat_2.5.branch0.bn.weight\n",
      "repeat_2.5.branch0.bn.bias\n",
      "repeat_2.5.branch1.0.conv.weight\n",
      "repeat_2.5.branch1.0.bn.weight\n",
      "repeat_2.5.branch1.0.bn.bias\n",
      "repeat_2.5.branch1.1.conv.weight\n",
      "repeat_2.5.branch1.1.bn.weight\n",
      "repeat_2.5.branch1.1.bn.bias\n",
      "repeat_2.5.branch1.2.conv.weight\n",
      "repeat_2.5.branch1.2.bn.weight\n",
      "repeat_2.5.branch1.2.bn.bias\n",
      "repeat_2.5.conv2d.weight\n",
      "repeat_2.5.conv2d.bias\n",
      "repeat_2.6.branch0.conv.weight\n",
      "repeat_2.6.branch0.bn.weight\n",
      "repeat_2.6.branch0.bn.bias\n",
      "repeat_2.6.branch1.0.conv.weight\n",
      "repeat_2.6.branch1.0.bn.weight\n",
      "repeat_2.6.branch1.0.bn.bias\n",
      "repeat_2.6.branch1.1.conv.weight\n",
      "repeat_2.6.branch1.1.bn.weight\n",
      "repeat_2.6.branch1.1.bn.bias\n",
      "repeat_2.6.branch1.2.conv.weight\n",
      "repeat_2.6.branch1.2.bn.weight\n",
      "repeat_2.6.branch1.2.bn.bias\n",
      "repeat_2.6.conv2d.weight\n",
      "repeat_2.6.conv2d.bias\n",
      "repeat_2.7.branch0.conv.weight\n",
      "repeat_2.7.branch0.bn.weight\n",
      "repeat_2.7.branch0.bn.bias\n",
      "repeat_2.7.branch1.0.conv.weight\n",
      "repeat_2.7.branch1.0.bn.weight\n",
      "repeat_2.7.branch1.0.bn.bias\n",
      "repeat_2.7.branch1.1.conv.weight\n",
      "repeat_2.7.branch1.1.bn.weight\n",
      "repeat_2.7.branch1.1.bn.bias\n",
      "repeat_2.7.branch1.2.conv.weight\n",
      "repeat_2.7.branch1.2.bn.weight\n",
      "repeat_2.7.branch1.2.bn.bias\n",
      "repeat_2.7.conv2d.weight\n",
      "repeat_2.7.conv2d.bias\n",
      "repeat_2.8.branch0.conv.weight\n",
      "repeat_2.8.branch0.bn.weight\n",
      "repeat_2.8.branch0.bn.bias\n",
      "repeat_2.8.branch1.0.conv.weight\n",
      "repeat_2.8.branch1.0.bn.weight\n",
      "repeat_2.8.branch1.0.bn.bias\n",
      "repeat_2.8.branch1.1.conv.weight\n",
      "repeat_2.8.branch1.1.bn.weight\n",
      "repeat_2.8.branch1.1.bn.bias\n",
      "repeat_2.8.branch1.2.conv.weight\n",
      "repeat_2.8.branch1.2.bn.weight\n",
      "repeat_2.8.branch1.2.bn.bias\n",
      "repeat_2.8.conv2d.weight\n",
      "repeat_2.8.conv2d.bias\n",
      "repeat_2.9.branch0.conv.weight\n",
      "repeat_2.9.branch0.bn.weight\n",
      "repeat_2.9.branch0.bn.bias\n",
      "repeat_2.9.branch1.0.conv.weight\n",
      "repeat_2.9.branch1.0.bn.weight\n",
      "repeat_2.9.branch1.0.bn.bias\n",
      "repeat_2.9.branch1.1.conv.weight\n",
      "repeat_2.9.branch1.1.bn.weight\n",
      "repeat_2.9.branch1.1.bn.bias\n",
      "repeat_2.9.branch1.2.conv.weight\n",
      "repeat_2.9.branch1.2.bn.weight\n",
      "repeat_2.9.branch1.2.bn.bias\n",
      "repeat_2.9.conv2d.weight\n",
      "repeat_2.9.conv2d.bias\n",
      "mixed_7a.branch0.0.conv.weight\n",
      "mixed_7a.branch0.0.bn.weight\n",
      "mixed_7a.branch0.0.bn.bias\n",
      "mixed_7a.branch0.1.conv.weight\n",
      "mixed_7a.branch0.1.bn.weight\n",
      "mixed_7a.branch0.1.bn.bias\n",
      "mixed_7a.branch1.0.conv.weight\n",
      "mixed_7a.branch1.0.bn.weight\n",
      "mixed_7a.branch1.0.bn.bias\n",
      "mixed_7a.branch1.1.conv.weight\n",
      "mixed_7a.branch1.1.bn.weight\n",
      "mixed_7a.branch1.1.bn.bias\n",
      "mixed_7a.branch2.0.conv.weight\n",
      "mixed_7a.branch2.0.bn.weight\n",
      "mixed_7a.branch2.0.bn.bias\n",
      "mixed_7a.branch2.1.conv.weight\n",
      "mixed_7a.branch2.1.bn.weight\n",
      "mixed_7a.branch2.1.bn.bias\n",
      "mixed_7a.branch2.2.conv.weight\n",
      "mixed_7a.branch2.2.bn.weight\n",
      "mixed_7a.branch2.2.bn.bias\n",
      "repeat_3.0.branch0.conv.weight\n",
      "repeat_3.0.branch0.bn.weight\n",
      "repeat_3.0.branch0.bn.bias\n",
      "repeat_3.0.branch1.0.conv.weight\n",
      "repeat_3.0.branch1.0.bn.weight\n",
      "repeat_3.0.branch1.0.bn.bias\n",
      "repeat_3.0.branch1.1.conv.weight\n",
      "repeat_3.0.branch1.1.bn.weight\n",
      "repeat_3.0.branch1.1.bn.bias\n",
      "repeat_3.0.branch1.2.conv.weight\n",
      "repeat_3.0.branch1.2.bn.weight\n",
      "repeat_3.0.branch1.2.bn.bias\n",
      "repeat_3.0.conv2d.weight\n",
      "repeat_3.0.conv2d.bias\n",
      "repeat_3.1.branch0.conv.weight\n",
      "repeat_3.1.branch0.bn.weight\n",
      "repeat_3.1.branch0.bn.bias\n",
      "repeat_3.1.branch1.0.conv.weight\n",
      "repeat_3.1.branch1.0.bn.weight\n",
      "repeat_3.1.branch1.0.bn.bias\n",
      "repeat_3.1.branch1.1.conv.weight\n",
      "repeat_3.1.branch1.1.bn.weight\n",
      "repeat_3.1.branch1.1.bn.bias\n",
      "repeat_3.1.branch1.2.conv.weight\n",
      "repeat_3.1.branch1.2.bn.weight\n",
      "repeat_3.1.branch1.2.bn.bias\n",
      "repeat_3.1.conv2d.weight\n",
      "repeat_3.1.conv2d.bias\n",
      "repeat_3.2.branch0.conv.weight\n",
      "repeat_3.2.branch0.bn.weight\n",
      "repeat_3.2.branch0.bn.bias\n",
      "repeat_3.2.branch1.0.conv.weight\n",
      "repeat_3.2.branch1.0.bn.weight\n",
      "repeat_3.2.branch1.0.bn.bias\n",
      "repeat_3.2.branch1.1.conv.weight\n",
      "repeat_3.2.branch1.1.bn.weight\n",
      "repeat_3.2.branch1.1.bn.bias\n",
      "repeat_3.2.branch1.2.conv.weight\n",
      "repeat_3.2.branch1.2.bn.weight\n",
      "repeat_3.2.branch1.2.bn.bias\n",
      "repeat_3.2.conv2d.weight\n",
      "repeat_3.2.conv2d.bias\n",
      "repeat_3.3.branch0.conv.weight\n",
      "repeat_3.3.branch0.bn.weight\n",
      "repeat_3.3.branch0.bn.bias\n",
      "repeat_3.3.branch1.0.conv.weight\n",
      "repeat_3.3.branch1.0.bn.weight\n",
      "repeat_3.3.branch1.0.bn.bias\n",
      "repeat_3.3.branch1.1.conv.weight\n",
      "repeat_3.3.branch1.1.bn.weight\n",
      "repeat_3.3.branch1.1.bn.bias\n",
      "repeat_3.3.branch1.2.conv.weight\n",
      "repeat_3.3.branch1.2.bn.weight\n",
      "repeat_3.3.branch1.2.bn.bias\n",
      "repeat_3.3.conv2d.weight\n",
      "repeat_3.3.conv2d.bias\n",
      "repeat_3.4.branch0.conv.weight\n",
      "repeat_3.4.branch0.bn.weight\n",
      "repeat_3.4.branch0.bn.bias\n",
      "repeat_3.4.branch1.0.conv.weight\n",
      "repeat_3.4.branch1.0.bn.weight\n",
      "repeat_3.4.branch1.0.bn.bias\n",
      "repeat_3.4.branch1.1.conv.weight\n",
      "repeat_3.4.branch1.1.bn.weight\n",
      "repeat_3.4.branch1.1.bn.bias\n",
      "repeat_3.4.branch1.2.conv.weight\n",
      "repeat_3.4.branch1.2.bn.weight\n",
      "repeat_3.4.branch1.2.bn.bias\n",
      "repeat_3.4.conv2d.weight\n",
      "repeat_3.4.conv2d.bias\n",
      "block8.branch0.conv.weight\n",
      "block8.branch0.bn.weight\n",
      "block8.branch0.bn.bias\n",
      "block8.branch1.0.conv.weight\n",
      "block8.branch1.0.bn.weight\n",
      "block8.branch1.0.bn.bias\n",
      "block8.branch1.1.conv.weight\n",
      "block8.branch1.1.bn.weight\n",
      "block8.branch1.1.bn.bias\n",
      "block8.branch1.2.conv.weight\n",
      "block8.branch1.2.bn.weight\n",
      "block8.branch1.2.bn.bias\n",
      "block8.conv2d.weight\n",
      "block8.conv2d.bias\n",
      "last_linear.weight\n",
      "last_bn.weight\n",
      "last_bn.bias\n",
      "logits.weight\n",
      "logits.bias\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1. Paths to your dataset\n",
    "    train_dir = \"cropped_mixed/train\"\n",
    "    train_csv = os.path.join(train_dir, \"train_labels.csv\")\n",
    "    val_dir = \"cropped_mixed/val\"\n",
    "    val_csv = os.path.join(val_dir, \"val_labels.csv\")\n",
    "\n",
    "    # 2. Augmentations\n",
    "    #    Heavier train augmentation, lighter val transform\n",
    "    train_transform = transforms.Compose([\n",
    "        # Example: random crop approach\n",
    "        transforms.Resize((180, 180)),       # make it bigger first\n",
    "        transforms.RandomCrop((160, 160)),   # then random crop to 160x160\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        # Random Erasing - can help generalize\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.1))\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 3. Datasets & DataLoaders\n",
    "    train_dataset = FaceDataset(train_dir, train_csv, transform=train_transform)\n",
    "    val_dataset = FaceDataset(val_dir, val_csv, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"Total training samples: {len(train_loader.dataset)}\")\n",
    "\n",
    "    # 4. Initialize model, loss, optimizer, device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "    if device == \"cuda\":\n",
    "        print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    # freeze_facenet=False to allow partial or full fine-tuning\n",
    "    model = FaceNetClassifier(num_emotions=8, freeze_facenet=False)\n",
    "\n",
    "    # Freeze all layers except the last few\n",
    "    for name, param in model.facenet.named_parameters():\n",
    "        # if \"last_bn\" in name or \"block7\" in name or \"block8\" in name:\n",
    "        #     param.requires_grad = True\n",
    "        # else:\n",
    "        #     param.requires_grad = False\n",
    "        param.requires_grad = True\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=1e-4\n",
    "    )\n",
    "\n",
    "    # 5. Train\n",
    "    print(\"Starting training...\")\n",
    "    trained_model, metrics = train_model(\n",
    "        model, criterion, optimizer, \n",
    "        train_loader, val_loader, \n",
    "        device, epochs=15\n",
    "    )\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    train_losses, train_accuracies, val_losses, val_accuracies = metrics\n",
    "\n",
    "    # Example plotting with matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    epochs_range = range(1, len(train_losses)+1)\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(epochs_range, train_accuracies, label='Train Acc')\n",
    "    plt.plot(epochs_range, val_accuracies, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Train vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # 6. Save your best model\n",
    "    torch.save(trained_model.state_dict(), \"facenet_emotion_classifier.pth\")\n",
    "    print(\"Model saved as facenet_emotion_classifier.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when freezing the last couple of layers (block7, block8, and last_bn) i was getting 69% (it was always less then 70%) and when i unfreeze all i started getting 75.4% and considering i have 17k images to train and strong data augmentation, unfreezing all of the pre trained model made more sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(model, image_path, device):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        _, pred = torch.max(logits, 1)\n",
    "    return pred.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m EMOTION_NAMES \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnger\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContempt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisgust\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHappy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeutral\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSad\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurprise\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m idx \u001b[38;5;241m=\u001b[39m predict_single_image(\u001b[43mtrained_model\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[0;32m      3\u001b[0m emotion_label \u001b[38;5;241m=\u001b[39m EMOTION_NAMES[idx]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted emotion:\u001b[39m\u001b[38;5;124m\"\u001b[39m, emotion_label)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "EMOTION_NAMES = [\"Anger\",\"Contempt\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\",\"Sad\",\"Surprise\"]\n",
    "idx = predict_single_image(trained_model, \"/path/to/image.jpg\", device)\n",
    "emotion_label = EMOTION_NAMES[idx]\n",
    "print(\"Predicted emotion:\", emotion_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
