{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition using a CNN model with integrated feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the AffectNET dataset from kaggle, unfortunately kaggle dont let you download the dataset to a specific location so i just moved it to my emotion_recognition folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\user\\.cache\\kagglehub\\datasets\\fatihkgg\\affectnet-yolo-format\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download the dataset\n",
    "try:\n",
    "    base_path = kagglehub.dataset_download(\"fatihkgg/affectnet-yolo-format\")\n",
    "    print(\"Path to dataset files:\", base_path)\n",
    "except Exception as e:\n",
    "    print(\"Error downloading dataset:\", e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset organization complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "source_base_path = os.path.join(base_path, \"YOLO_format\")  # Add YOLO_format to the downloaded path\n",
    "destination_base_path = os.path.join(os.getcwd(), \"AffectNET\")  # Place organized data in 'AffectNET' folder\n",
    "\n",
    "# Define emotions\n",
    "emotions = [\n",
    "    \"Anger\",\n",
    "    \"Contempt\",\n",
    "    \"Disgust\",\n",
    "    \"Fear\",\n",
    "    \"Happy\",\n",
    "    \"Neutral\",\n",
    "    \"Sad\",\n",
    "    \"Surprise\",\n",
    "]\n",
    "\n",
    "# Define function to organize files\n",
    "def organize_affectnet(source_base_path, destination_base_path, split):\n",
    "    source_images_path = os.path.join(source_base_path, split, \"images\")\n",
    "    source_labels_path = os.path.join(source_base_path, split, \"labels\")\n",
    "\n",
    "    if not os.path.exists(source_images_path) or not os.path.exists(source_labels_path):\n",
    "        print(f\"Warning: {split} images or labels path does not exist. Skipping.\")\n",
    "        return\n",
    "\n",
    "    destination_split_path = os.path.join(destination_base_path, f\"{split}_organized\")\n",
    "\n",
    "    # Create destination directories for emotions\n",
    "    for emotion in emotions:\n",
    "        emotion_folder = os.path.join(destination_split_path, emotion)\n",
    "        os.makedirs(emotion_folder, exist_ok=True)\n",
    "\n",
    "    # Move and organize files\n",
    "    for label_file in os.listdir(source_labels_path):\n",
    "        if label_file.endswith(\".txt\"):\n",
    "            label_path = os.path.join(source_labels_path, label_file)\n",
    "\n",
    "            # Handle both .jpg and .png image extensions\n",
    "            image_file_jpg = label_file.replace(\".txt\", \".jpg\")\n",
    "            image_file_png = label_file.replace(\".txt\", \".png\")\n",
    "\n",
    "            source_image_path_jpg = os.path.join(source_images_path, image_file_jpg)\n",
    "            source_image_path_png = os.path.join(source_images_path, image_file_png)\n",
    "\n",
    "            if os.path.exists(source_image_path_jpg) or os.path.exists(source_image_path_png):\n",
    "                source_image_path = source_image_path_jpg if os.path.exists(source_image_path_jpg) else source_image_path_png\n",
    "\n",
    "                with open(label_path, \"r\") as f:\n",
    "                    labels = f.readlines()\n",
    "\n",
    "                for label in labels:\n",
    "                    label_data = label.strip().split()\n",
    "                    emotion_index = int(label_data[0])  # Emotion class index\n",
    "                    if emotion_index < len(emotions):\n",
    "                        emotion = emotions[emotion_index]\n",
    "                        destination_image_path = os.path.join(destination_split_path, emotion, os.path.basename(source_image_path))\n",
    "                        shutil.copy(source_image_path, destination_image_path)\n",
    "                        break  # Move the image based on the first valid label\n",
    "            else:\n",
    "                print(f\"Image {image_file_jpg} or {image_file_png} not found for label {label_file}\")\n",
    "\n",
    "# Create the main AffectNET directory\n",
    "os.makedirs(destination_base_path, exist_ok=True)\n",
    "\n",
    "# Organize train, valid, and test datasets\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    organize_affectnet(source_base_path, destination_base_path, split)\n",
    "\n",
    "print(\"Dataset organization complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "- Organizes the AffectNet dataset downloaded via KaggleHub into a structured format compatible with the emotion recognition program.\n",
    "- Moves images into emotion-specific folders (`Anger`, `Happy`, etc.) for training, validation, and testing.\n",
    "\n",
    "### How it Works\n",
    "1. **Defines Input and Output Paths**:\n",
    "   - Takes the original KaggleHub dataset location as `base_path`.\n",
    "   - Organizes the files into an `AffectNET` folder in the current working directory (`destination_base_path`).\n",
    "2. **Emotions Mapping**:\n",
    "   - Maps labels from `labels` files to emotion folder names (`Anger`, `Happy`, etc.).\n",
    "3. **File Organization**:\n",
    "   - Reads label files (`.txt`) to identify the emotion associated with each image.\n",
    "   - Copies images into their respective emotion folders under `train_organized`, `valid_organized`, and `test_organized`.\n",
    "4. **Error Handling**:\n",
    "   - Skips missing files and directories, providing warnings in the output.\n",
    "\n",
    "### Returns\n",
    "- Creates a folder structure:\n",
    "  ```\n",
    "  AffectNET/\n",
    "    train_organized/\n",
    "      Anger/\n",
    "      Contempt/\n",
    "      ...\n",
    "    valid_organized/\n",
    "      Anger/\n",
    "      Contempt/\n",
    "      ...\n",
    "    test_organized/\n",
    "      Anger/\n",
    "      Contempt/\n",
    "      ...\n",
    "  ```\n",
    "- Organizes all images and labels into their respective emotion folders for easy use in machine learning pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Explanation\n",
    "\n",
    "## Imports\n",
    "- **`os`**: Used for interacting with the file system (e.g., navigating directories, managing file paths).\n",
    "- **`tensorflow`**: A deep learning library used for building and training machine learning models.\n",
    "  - **`Model`**: Represents a neural network model in Keras.\n",
    "  - **`load_model`**: Loads a saved model from a file.\n",
    "  - **Layers**:\n",
    "    - **`Conv2D`**: Convolutional layer for feature extraction.\n",
    "    - **`MaxPooling2D`**: Down-samples feature maps.\n",
    "    - **`Flatten`**: Converts multi-dimensional data into a 1D vector.\n",
    "    - **`Dense`**: Fully connected layer for classification.\n",
    "    - **`Input`**: Defines the input layer of the model.\n",
    "    - **`BatchNormalization`**: Normalizes layer outputs for stable training.\n",
    "- **`numpy`**: Provides numerical operations for array manipulation (used for preprocessing and dataset preparation).\n",
    "- **`matplotlib.pyplot`**: Used for visualizing images and feature heatmaps.\n",
    "- **`cv2` (OpenCV)**: Handles image processing and real-time camera feed.\n",
    "- **`random`**: Used for generating random numbers and selections.\n",
    "- **`logging`**: Configures logging behavior (e.g., suppresses TensorFlow logs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Flatten, Dense, Input, BatchNormalization\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**: Limits TensorFlow logs to show only errors, avoiding clutter in the output.\n",
    "\n",
    "**How it works**: Sets TensorFlow’s logging level to ERROR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow progress bar\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, train_path, valid_path, test_path, emotions):\n",
    "        self.train_path = train_path\n",
    "        self.valid_path = valid_path\n",
    "        self.test_path = test_path\n",
    "        self.emotions = emotions\n",
    "\n",
    "    def load_affectnet_dataset(self, dataset_path, image_size=(128, 128)):\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for label, emotion in enumerate(self.emotions):\n",
    "            folder_path = os.path.join(dataset_path, emotion)\n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"Warning: Folder {folder_path} does not exist. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Loading images from: {folder_path}\")\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".png\"):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    try:\n",
    "                        image = cv2.imread(file_path)\n",
    "                        if image is None:\n",
    "                            print(f\"Warning: Unable to load image {file_path}. Skipping.\")\n",
    "                            continue\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                        image = cv2.resize(image, image_size)\n",
    "                        images.append(image)\n",
    "                        labels.append(label)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading image {file_path}: {e}\")\n",
    "\n",
    "        images = np.array(images, dtype=\"float32\") / 255.0\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        print(f\"Loaded {len(images)} images from {dataset_path}.\")\n",
    "        return images, labels\n",
    "\n",
    "    def prepare_datasets(self):\n",
    "        X_train, y_train = self.load_affectnet_dataset(self.train_path)\n",
    "        X_val, y_val = self.load_affectnet_dataset(self.valid_path)\n",
    "        X_test, y_test = self.load_affectnet_dataset(self.test_path)\n",
    "\n",
    "        y_train = to_categorical(y_train, num_classes=len(self.emotions))\n",
    "        y_val = to_categorical(y_val, num_classes=len(self.emotions))\n",
    "        y_test = to_categorical(y_test, num_classes=len(self.emotions))\n",
    "\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Purpose**: Loads the train, validation, and test datasets and converts their labels to one-hot encoding.\n",
    "- **How it Works**:\n",
    "  1. Calls `load_affectnet_dataset` for each dataset (train, validation, test).\n",
    "  2. Converts the integer labels to one-hot encoded format using `to_categorical`.\n",
    "\n",
    "- **Returns**:\n",
    "  - `X_train`, `y_train`: Training images and one-hot encoded labels.\n",
    "  - `X_val`, `y_val`: Validation images and one-hot encoded labels.\n",
    "  - `X_test`, `y_test`: Testing images and one-hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN:\n",
    "    def __init__(self, emotions):\n",
    "        self.emotions = emotions\n",
    "        self.feature_extractor = None\n",
    "        self.emotion_classifier = None\n",
    "        self.combined_model = None\n",
    "\n",
    "    def build_feature_extractor(self, input_shape=(128, 128, 3)):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        x = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        x = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        self.feature_extractor = Model(inputs, x, name=\"FeatureExtractor\")\n",
    "        return self.feature_extractor\n",
    "\n",
    "    def build_emotion_classifier(self, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Dense(256, activation=\"relu\")(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(128, activation=\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        outputs = Dense(len(self.emotions), activation=\"softmax\")(x)\n",
    "        self.emotion_classifier = Model(inputs, outputs, name=\"EmotionClassifier\")\n",
    "        return self.emotion_classifier\n",
    "\n",
    "    def build_combined_model(self):\n",
    "        inputs = Input(shape=(128, 128, 3))\n",
    "        features = self.feature_extractor(inputs)\n",
    "        outputs = self.emotion_classifier(features)\n",
    "        self.combined_model = Model(inputs, outputs, name=\"EmotionRecognitionModel\")\n",
    "        return self.combined_model\n",
    "\n",
    "    def compile_and_train(self, X_train, y_train, X_val, y_val, model_save_path, epochs=10, batch_size=32):\n",
    "        if os.path.exists(model_save_path):\n",
    "            print(\"Loading saved model...\")\n",
    "            self.combined_model = load_model(model_save_path)\n",
    "        else:\n",
    "            print(\"No saved model found. Creating a new model...\")\n",
    "            self.build_feature_extractor()\n",
    "            self.build_emotion_classifier(self.feature_extractor.output_shape[-1])\n",
    "            self.build_combined_model()\n",
    "\n",
    "            self.combined_model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"]\n",
    "            )\n",
    "            self.combined_model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            self.combined_model.save(model_save_path)\n",
    "            print(f\"Model saved at {model_save_path}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the EmotionCNN Class\n",
    "\n",
    "### Purpose\n",
    "The `EmotionCNN` class is responsible for building, training, and managing a convolutional neural network (CNN) for emotion recognition. It comprises three main components:\n",
    "- A feature extractor to process images and extract relevant features.\n",
    "- An emotion classifier to predict emotions based on extracted features.\n",
    "- A combined model that integrates both the feature extractor and emotion classifier for end-to-end training and inference.\n",
    "\n",
    "### Methods\n",
    "\n",
    "#### 1. **`__init__` Method**\n",
    "Initializes the `EmotionCNN` class and sets up placeholders for the feature extractor, emotion classifier, and combined model.\n",
    "- **Parameters**: `emotions` (list of emotion labels).\n",
    "\n",
    "#### 2. **`build_feature_extractor` Method**\n",
    "Builds the feature extractor part of the CNN, which extracts meaningful features from input images.\n",
    "- **Purpose**: To create a convolutional network with three convolutional layers, each followed by batch normalization and max-pooling.\n",
    "- **Input**: Image tensor of shape `(128, 128, 3)`.\n",
    "- **How It Works**:\n",
    "  - Convolutional layers (`Conv2D`) extract spatial features.\n",
    "  - `BatchNormalization` stabilizes training and improves convergence.\n",
    "  - `MaxPooling2D` reduces spatial dimensions to focus on high-level features.\n",
    "  - `Flatten` prepares the output for fully connected layers.\n",
    "- **Output**: A Keras `Model` object representing the feature extractor.\n",
    "\n",
    "#### 3. **`build_emotion_classifier` Method**\n",
    "Builds the emotion classifier, which maps extracted features to emotion labels.\n",
    "- **Purpose**: To create a fully connected network for classification.\n",
    "- **Input**: Flattened feature tensor.\n",
    "- **How It Works**:\n",
    "  - Dense layers (`Dense`) process extracted features with `ReLU` activations.\n",
    "  - `BatchNormalization` improves stability.\n",
    "  - Final dense layer outputs probabilities for each emotion using `softmax` activation.\n",
    "- **Output**: A Keras `Model` object representing the emotion classifier.\n",
    "\n",
    "#### 4. **`build_combined_model` Method**\n",
    "Combines the feature extractor and emotion classifier into a single end-to-end model.\n",
    "- **Purpose**: To allow training and inference in a unified model.\n",
    "- **How It Works**: The feature extractor processes input images, and the emotion classifier predicts emotions based on the extracted features.\n",
    "- **Output**: A Keras `Model` object representing the combined model.\n",
    "\n",
    "#### 5. **`compile_and_train` Method**\n",
    "Compiles and trains the combined model or loads a pre-trained model if available.\n",
    "- **Purpose**: To train the model on the provided dataset or use a pre-trained model for further evaluation.\n",
    "- **Parameters**:\n",
    "  - `X_train`, `y_train`: Training data and labels.\n",
    "  - `X_val`, `y_val`: Validation data and labels.\n",
    "  - `model_save_path`: Path to save the trained model.\n",
    "  - `epochs`, `batch_size`: Training hyperparameters.\n",
    "- **How It Works**:\n",
    "  - Checks if a pre-trained model exists at `model_save_path`.\n",
    "  - If not, builds the feature extractor, emotion classifier, and combined model.\n",
    "  - Compiles the model using the Adam optimizer and categorical crossentropy loss.\n",
    "  - Trains the model using the provided dataset.\n",
    "  - Saves the trained model to `model_save_path`.\n",
    "- **Returns**: Trained model (or loaded model if already available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction:\n",
    "    @staticmethod\n",
    "    def visualize_features(image, feature_extractor):\n",
    "        features = feature_extractor.predict(np.expand_dims(image, axis=0), verbose=0)\n",
    "        feature_map = features.reshape((16, 16, -1))\n",
    "        feature_map_aggregated = np.mean(feature_map, axis=-1)\n",
    "        feature_map_normalized = (feature_map_aggregated - feature_map_aggregated.min()) / (\n",
    "            feature_map_aggregated.max() - feature_map_aggregated.min()\n",
    "        )\n",
    "        feature_map_rescaled = cv2.resize(feature_map_normalized, (128, 128))\n",
    "        heatmap = cv2.applyColorMap((feature_map_rescaled * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        overlay_image = cv2.addWeighted((image * 255).astype(np.uint8), 0.6, heatmap, 0.4, 0)\n",
    "        return overlay_image, heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the FeatureExtraction Class\n",
    "\n",
    "### Purpose\n",
    "The `FeatureExtraction` class provides a static method to visualize features extracted by a feature extractor model. This is helpful for understanding what parts of an input image the model focuses on during processing.\n",
    "\n",
    "### Methods\n",
    "\n",
    "#### **`visualize_features` Method**\n",
    "Visualizes the features extracted from an image by creating a heatmap overlay.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `image`: Input image (assumed to be preprocessed and resized to match the model's input shape).\n",
    "  - `feature_extractor`: A pre-trained feature extractor model.\n",
    "\n",
    "- **How It Works**:\n",
    "  1. **Feature Extraction**:\n",
    "     - Passes the image through the `feature_extractor` to obtain the feature map.\n",
    "     - The feature map is reshaped to its original dimensions (e.g., `16x16xN` for a 16x16 spatial resolution with `N` channels).\n",
    "  2. **Aggregation**:\n",
    "     - Aggregates the feature map across channels by taking the mean, resulting in a 2D spatial representation.\n",
    "  3. **Normalization**:\n",
    "     - Normalizes the aggregated feature map to the range `[0, 1]` for visualization.\n",
    "  4. **Resizing**:\n",
    "     - Rescales the normalized feature map to the original image size (`128x128` by default).\n",
    "  5. **Heatmap Generation**:\n",
    "     - Converts the feature map into a heatmap using OpenCV’s `COLORMAP_JET`.\n",
    "     - Converts the heatmap from BGR to RGB format.\n",
    "  6. **Overlay Creation**:\n",
    "     - Blends the heatmap with the original image to create an overlay that highlights regions of high activation.\n",
    "\n",
    "- **Returns**:\n",
    "  - `overlay_image`: The input image with the heatmap overlaid.\n",
    "  - `heatmap`: The generated heatmap, useful for standalone visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDetectionSystem:\n",
    "    def __init__(self, train_path, valid_path, test_path, emotions, model_save_path):\n",
    "        self.data_processor = DataProcessor(train_path, valid_path, test_path, emotions)\n",
    "        self.emotion_cnn = EmotionCNN(emotions)\n",
    "        self.model_save_path = model_save_path\n",
    "\n",
    "    def run(self):\n",
    "        # Prepare data\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = self.data_processor.prepare_datasets()\n",
    "\n",
    "        # Train or load model\n",
    "        self.emotion_cnn.compile_and_train(X_train, y_train, X_val, y_val, self.model_save_path)\n",
    "\n",
    "        # Evaluate model\n",
    "        test_loss, test_accuracy = self.emotion_cnn.combined_model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "        # Live feed for emotion detection\n",
    "        self.live_camera_feed()\n",
    "\n",
    "    def live_camera_feed(self):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Camera not found or cannot be opened.\")\n",
    "            return\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "        cv2.namedWindow(\"Emotion Recognition\", cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\"Emotion Recognition\", 1280, 720)\n",
    "\n",
    "        print(\"Press 'q' to quit.\")\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Unable to read frame from camera.\")\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_normalized = frame_rgb / 255.0\n",
    "            frame_resized = cv2.resize(frame_normalized, (128, 128))\n",
    "\n",
    "            prediction = self.emotion_cnn.combined_model.predict(np.expand_dims(frame_resized, axis=0), verbose=0)\n",
    "            predicted_emotion = self.data_processor.emotions[np.argmax(prediction)]\n",
    "\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"Emotion: {predicted_emotion}\",\n",
    "                (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "            cv2.imshow(\"Emotion Recognition\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q') or cv2.getWindowProperty(\"Emotion Recognition\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the EmotionDetectionSystem Class\n",
    "\n",
    "### Purpose\n",
    "The `EmotionDetectionSystem` class integrates data preparation, model training, evaluation, and live camera-based emotion detection into a single system. It acts as the main interface for running the emotion recognition pipeline.\n",
    "\n",
    "### Methods\n",
    "\n",
    "#### **`__init__` Method**\n",
    "Initializes the `EmotionDetectionSystem` class and sets up the components.\n",
    "- **Parameters**:\n",
    "  - `train_path`, `valid_path`, `test_path`: Paths to the training, validation, and testing datasets.\n",
    "  - `emotions`: List of emotion labels.\n",
    "  - `model_save_path`: File path to save or load the trained model.\n",
    "- **Components Initialized**:\n",
    "  - `DataProcessor`: Handles data loading and preprocessing.\n",
    "  - `EmotionCNN`: Builds, trains, and manages the emotion recognition CNN.\n",
    "\n",
    "#### **`run` Method**\n",
    "Executes the full pipeline: data preparation, model training/loading, evaluation, and live camera detection.\n",
    "- **Steps**:\n",
    "  1. Prepares datasets using `DataProcessor.prepare_datasets`.\n",
    "  2. Trains or loads the emotion recognition model via `EmotionCNN.compile_and_train`.\n",
    "  3. Evaluates the trained model on the test dataset and prints the test accuracy.\n",
    "  4. Launches the live camera feed for real-time emotion detection.\n",
    "\n",
    "#### **`live_camera_feed` Method**\n",
    "Captures video from a webcam and performs real-time emotion recognition.\n",
    "- **How It Works**:\n",
    "  1. **Camera Initialization**:\n",
    "     - Opens the webcam using OpenCV (`cv2.VideoCapture`).\n",
    "     - Sets the video resolution to 1280x720.\n",
    "  2. **Frame Processing**:\n",
    "     - Captures each frame from the webcam.\n",
    "     - Converts the frame to RGB format and normalizes pixel values.\n",
    "     - Resizes the frame to match the model input size (128x128).\n",
    "  3. **Emotion Prediction**:\n",
    "     - Passes the preprocessed frame through the trained model to get emotion predictions.\n",
    "     - Extracts the predicted emotion label.\n",
    "  4. **Display**:\n",
    "     - Overlays the predicted emotion label on the video feed using `cv2.putText`.\n",
    "     - Displays the video feed in a resizable OpenCV window.\n",
    "  5. **Exit Conditions**:\n",
    "     - Closes the video feed when the user presses 'q' or clicks the close button on the window.\n",
    "- **Error Handling**:\n",
    "  - If the camera cannot be opened, an error message is printed.\n",
    "  - Handles cases where frames cannot be read from the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: ./AffectNET/train_organized\\Anger\n",
      "Loading images from: ./AffectNET/train_organized\\Contempt\n",
      "Loading images from: ./AffectNET/train_organized\\Disgust\n",
      "Loading images from: ./AffectNET/train_organized\\Fear\n",
      "Loading images from: ./AffectNET/train_organized\\Happy\n",
      "Loading images from: ./AffectNET/train_organized\\Neutral\n",
      "Loading images from: ./AffectNET/train_organized\\Sad\n",
      "Loading images from: ./AffectNET/train_organized\\Surprise\n",
      "Loaded 4934 images from ./AffectNET/train_organized.\n",
      "Loading images from: ./AffectNET/valid_organized\\Anger\n",
      "Loading images from: ./AffectNET/valid_organized\\Contempt\n",
      "Loading images from: ./AffectNET/valid_organized\\Disgust\n",
      "Loading images from: ./AffectNET/valid_organized\\Fear\n",
      "Loading images from: ./AffectNET/valid_organized\\Happy\n",
      "Loading images from: ./AffectNET/valid_organized\\Neutral\n",
      "Loading images from: ./AffectNET/valid_organized\\Sad\n",
      "Loading images from: ./AffectNET/valid_organized\\Surprise\n",
      "Loaded 1860 images from ./AffectNET/valid_organized.\n",
      "Loading images from: ./AffectNET/test_organized\\Anger\n",
      "Loading images from: ./AffectNET/test_organized\\Contempt\n",
      "Loading images from: ./AffectNET/test_organized\\Disgust\n",
      "Loading images from: ./AffectNET/test_organized\\Fear\n",
      "Loading images from: ./AffectNET/test_organized\\Happy\n",
      "Loading images from: ./AffectNET/test_organized\\Neutral\n",
      "Loading images from: ./AffectNET/test_organized\\Sad\n",
      "Loading images from: ./AffectNET/test_organized\\Surprise\n",
      "Loaded 1003 images from ./AffectNET/test_organized.\n",
      "No saved model found. Creating a new model...\n",
      "Epoch 1/10\n",
      "155/155 [==============================] - 7s 24ms/step - loss: 1.7795 - accuracy: 0.3944 - val_loss: 2.5416 - val_accuracy: 0.1855\n",
      "Epoch 2/10\n",
      "155/155 [==============================] - 3s 22ms/step - loss: 1.1433 - accuracy: 0.6001 - val_loss: 2.1860 - val_accuracy: 0.3086\n",
      "Epoch 3/10\n",
      "155/155 [==============================] - 3s 21ms/step - loss: 0.8934 - accuracy: 0.6834 - val_loss: 1.9404 - val_accuracy: 0.3613\n",
      "Epoch 4/10\n",
      "155/155 [==============================] - 3s 21ms/step - loss: 0.6028 - accuracy: 0.7892 - val_loss: 1.2518 - val_accuracy: 0.5710\n",
      "Epoch 5/10\n",
      "155/155 [==============================] - 3s 21ms/step - loss: 0.3526 - accuracy: 0.8887 - val_loss: 1.3860 - val_accuracy: 0.5457\n",
      "Epoch 6/10\n",
      "155/155 [==============================] - 3s 21ms/step - loss: 0.1746 - accuracy: 0.9534 - val_loss: 1.5323 - val_accuracy: 0.5591\n",
      "Epoch 7/10\n",
      "155/155 [==============================] - 3s 21ms/step - loss: 0.0838 - accuracy: 0.9805 - val_loss: 1.5631 - val_accuracy: 0.5602\n",
      "Epoch 8/10\n",
      "155/155 [==============================] - 3s 21ms/step - loss: 0.0432 - accuracy: 0.9909 - val_loss: 1.5699 - val_accuracy: 0.5871\n",
      "Epoch 9/10\n",
      "155/155 [==============================] - 3s 21ms/step - loss: 0.0658 - accuracy: 0.9820 - val_loss: 2.2290 - val_accuracy: 0.4618\n",
      "Epoch 10/10\n",
      "155/155 [==============================] - 3s 21ms/step - loss: 0.3159 - accuracy: 0.8867 - val_loss: 1.9771 - val_accuracy: 0.5559\n",
      "Model saved at emotion_recognition_model.h5\n",
      "Test Accuracy: 55.13%\n",
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    TRAIN_PATH = \"./AffectNET/train_organized\"\n",
    "    VALID_PATH = \"./AffectNET/valid_organized\"\n",
    "    TEST_PATH = \"./AffectNET/test_organized\"\n",
    "    EMOTIONS = [\"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "    MODEL_SAVE_PATH = \"emotion_recognition_model.h5\"\n",
    "\n",
    "    system = EmotionDetectionSystem(TRAIN_PATH, VALID_PATH, TEST_PATH, EMOTIONS, MODEL_SAVE_PATH)\n",
    "    system.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
